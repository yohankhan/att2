{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoHu5Vj0N-tk",
        "outputId": "01139ddd-787a-4836-ee8d-989a011f9626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Unigram tokenizer...\n",
            "Tokenizer training complete! Files: sp_unigram_25k.model / sp_unigram_25k.vocab\n",
            "\n",
            "Evaluation Results:\n",
            "Average tokens per word: 1.2885\n",
            "Max tokens in a line: 23\n",
            "Reconstruction rate: 0.9606\n",
            "\n",
            "✅ Tokenizer saved locally in Colab as: sp_unigram_25k.model and sp_unigram_25k.vocab\n",
            "You can download them using:\n",
            "files.download('sp_unigram_25k.model')\n",
            "files.download('sp_unigram_25k.vocab')\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# !pip install sentencepiece pandas\n",
        "# !pip install tokenizers  transformers datasets\n",
        "\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "import unicodedata\n",
        "\n",
        "input_file = \"raw.txt\"\n",
        "output_file = \"shakespeare_clean.txt\"\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Unicode normalization (NFKC)\n",
        "text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "# Keep line breaks and punctuation (important for poetic structure)\n",
        "# Optional: strip trailing whitespaces\n",
        "lines = [line.rstrip() for line in text.splitlines() if line.strip()]\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "# -------------------------------\n",
        "# PARAMETERS YOU CAN TWEAK\n",
        "# -------------------------------\n",
        "INPUT_FILE = \"shakespeare_clean.txt\"  # path to your text file\n",
        "MODEL_PREFIX = \"sp_unigram_25k\"       # model name prefix\n",
        "VOCAB_SIZE = 25000                     # increase to 32000, 50000 for larger vocab\n",
        "MODEL_TYPE = \"unigram\"                # \"unigram\" or \"bpe\"\n",
        "CHARACTER_COVERAGE = 0.9995            # 0.99 for most English text\n",
        "INPUT_SENTENCE_SIZE = 2000000          # number of sentences to sample for training\n",
        "SHUFFLE = True                         # shuffle sentences before training\n",
        "N_EVAL_LINES = 5000                     # number of lines to use for quick evaluation\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 1: Train Tokenizer\n",
        "# -------------------------------\n",
        "print(\"Training Unigram tokenizer...\")\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=INPUT_FILE,\n",
        "    model_prefix=MODEL_PREFIX,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type=MODEL_TYPE,\n",
        "    character_coverage=CHARACTER_COVERAGE,\n",
        "    input_sentence_size=INPUT_SENTENCE_SIZE,\n",
        "    shuffle_input_sentence=SHUFFLE\n",
        ")\n",
        "print(f\"Tokenizer training complete! Files: {MODEL_PREFIX}.model / {MODEL_PREFIX}.vocab\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 2: Load Tokenizer\n",
        "# -------------------------------\n",
        "sp_tokenizer = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 3: Evaluation Function\n",
        "# -------------------------------\n",
        "def evaluate_tokenizer(sp_model, text_path, n_lines=None):\n",
        "    total_tokens, total_words, recon_errors, max_len = 0, 0, 0, 0\n",
        "    with open(text_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines and i >= n_lines:\n",
        "                break\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            enc = sp_model.encode(line, out_type=int)\n",
        "            decoded = sp_model.decode(enc).strip()\n",
        "            if decoded != line.strip():\n",
        "                recon_errors += 1\n",
        "            total_tokens += len(enc)\n",
        "            total_words += len(line.split())\n",
        "            max_len = max(max_len, len(enc))\n",
        "    return {\n",
        "        \"avg_tokens_per_word\": total_tokens / (total_words + 1e-9),\n",
        "        \"max_tokens_in_line\": max_len,\n",
        "        \"reconstruction_rate\": 1 - recon_errors / (i+1e-9)\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 4: Run Evaluation\n",
        "# -------------------------------\n",
        "results = evaluate_tokenizer(sp_tokenizer, INPUT_FILE, n_lines=N_EVAL_LINES)\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Average tokens per word: {results['avg_tokens_per_word']:.4f}\")\n",
        "print(f\"Max tokens in a line: {results['max_tokens_in_line']}\")\n",
        "print(f\"Reconstruction rate: {results['reconstruction_rate']:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 5: Save Tokenizer Locally in Colab\n",
        "# -------------------------------\n",
        "print(f\"\\n✅ Tokenizer saved locally in Colab as: {MODEL_PREFIX}.model and {MODEL_PREFIX}.vocab\")\n",
        "print(\"You can download them using:\")\n",
        "print(f\"files.download('{MODEL_PREFIX}.model')\")\n",
        "print(f\"files.download('{MODEL_PREFIX}.vocab')\")\n",
        "# 1. Average Tokens per Word\n",
        "\n",
        "# What it tells you: How efficiently the tokenizer compresses text into tokens.\n",
        "\n",
        "# Lower is better → fewer tokens per word means your model will process longer context for the same input size.\n",
        "\n",
        "# Compare: If your new tokenizer has lower or similar avg tokens/word than 1.3085, it’s more efficient or equally efficient.\n",
        "\n",
        "# 2. Max Tokens per Line\n",
        "\n",
        "# What it tells you: The longest tokenized sequence in a single line.\n",
        "\n",
        "# Lower is generally better → prevents very long sequences that could slow training or require truncation.\n",
        "\n",
        "# Compare: If your new tokenizer reduces this from 24, it’s better in sequence efficiency.\n",
        "\n",
        "# 3. Reconstruction Rate\n",
        "\n",
        "# What it tells you: How accurately the tokenizer can reproduce the original text after encoding → decoding.\n",
        "\n",
        "# Higher is better → close to 1 (or 100%) means no loss of information.\n",
        "\n",
        "# Compare: Your current tokenizer has 0.9606 → new tokenizer should ideally be ≥ 0.9606.\n",
        "\n",
        "# Summary Table for Evaluation\n",
        "# Metric\tBetter Condition\n",
        "# Avg Tokens per Word\tLower than 1.3085\n",
        "# Max Tokens per Line\tLower than 24\n",
        "# Reconstruction Rate\tEqual to or higher than 0.9606\n",
        "\n",
        "# ✅ Rule of Thumb:\n",
        "\n",
        "# If average tokens per word decreases without dropping reconstruction rate, your tokenizer is better.\n",
        "\n",
        "# Slight increase in max tokens per line is okay if reconstruction improves or avg tokens per word decreases.\n",
        "\n",
        "# If reconstruction rate drops significantly, the tokenizer is losing fidelity, even if token counts improve.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade transformers --quiet\n"
      ],
      "metadata": {
        "id": "sE3mbqiKReOD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ===============================\n",
        "# !pip install torch tqdm sentencepiece\n",
        "\n",
        "# ===============================\n",
        "# IMPORTS\n",
        "# ===============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sentencepiece as spm\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# PARAMETERS (Tweak for experiments)\n",
        "# ===============================\n",
        "TEXT_FILE = \"shakespeare_clean.txt\"\n",
        "TOKENIZER_MODEL = \"sp_unigram_25k.model\"\n",
        "\n",
        "VOCAB_SIZE = 25000 + 3       # +3 for PAD=0, BOS=1, EOS=2\n",
        "PAD_ID = 0\n",
        "BOS_ID = 1\n",
        "EOS_ID = 2\n",
        "\n",
        "BLOCK_SIZE = 128\n",
        "EMBED_DIM = 512\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "FF_DIM = 2048\n",
        "DROPOUT = 0.1\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "NUM_EPOCHS = 1\n",
        "LEARNING_RATE = 5e-4\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ===============================\n",
        "# STEP 1: LOAD TOKENIZER\n",
        "# ===============================\n",
        "sp_tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL)\n",
        "\n",
        "# Safe encoding to offset IDs for special tokens\n",
        "def encode_line_safe(line):\n",
        "    ids = sp_tokenizer.encode(line, out_type=int)\n",
        "    ids = [id + 3 for id in ids]  # shift all normal tokens by 3\n",
        "    return [BOS_ID] + ids + [EOS_ID]\n",
        "\n",
        "# ===============================\n",
        "# STEP 2: CUSTOM DATASET\n",
        "# ===============================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, block_size):\n",
        "        self.block_size = block_size\n",
        "        self.lines = []\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    token_ids = encode_line_safe(line)\n",
        "                    if len(token_ids) > 1:  # skip empty sequences\n",
        "                        self.lines.append(token_ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.lines[idx]\n",
        "        if len(ids) > self.block_size:\n",
        "            ids = ids[:self.block_size]\n",
        "        x = torch.tensor(ids[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(ids[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "dataset = TextDataset(TEXT_FILE, BLOCK_SIZE)\n",
        "train_size = int(0.95 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    x_batch, y_batch = zip(*batch)\n",
        "    # Find max length in batch\n",
        "    max_len = max([len(x) for x in x_batch])\n",
        "    # Pad sequences\n",
        "    x_padded = torch.full((len(batch), max_len), PAD_ID, dtype=torch.long)\n",
        "    y_padded = torch.full((len(batch), max_len), PAD_ID, dtype=torch.long)\n",
        "    for i in range(len(batch)):\n",
        "        x_padded[i, :len(x_batch[i])] = x_batch[i]\n",
        "        y_padded[i, :len(y_batch[i])] = y_batch[i]\n",
        "    return x_padded, y_padded\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
        "\n",
        "# ===============================\n",
        "# STEP 3: TRANSFORMER MODEL\n",
        "# ===============================\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_dim, block_size, dropout=0.1, pad_id=0):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads,\n",
        "                                                   dim_feedforward=ff_dim, dropout=dropout, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, t = x.size()\n",
        "        positions = torch.arange(t, device=x.device).unsqueeze(0).expand(b, t)\n",
        "        x = self.token_emb(x) + self.pos_emb(positions)\n",
        "        x = self.transformer(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "model = TransformerLM(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, FF_DIM, BLOCK_SIZE, DROPOUT, PAD_ID).to(DEVICE)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "# ===============================\n",
        "# STEP 4: TRAINING LOOP\n",
        "# ===============================\n",
        "scaler = torch.amp.GradScaler() if DEVICE==\"cuda\" else None\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for xb, yb in loop:\n",
        "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(device_type='cuda', enabled=(DEVICE==\"cuda\")):\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits.view(-1, VOCAB_SIZE), yb.view(-1))\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "# ===============================\n",
        "# STEP 5: EVALUATION & SAMPLE GENERATION\n",
        "# ===============================\n",
        "def generate_text(model, tokenizer, prompt, max_len=BLOCK_SIZE, temperature=1.0, top_k=50):\n",
        "    model.eval()\n",
        "    ids = encode_line_safe(prompt)\n",
        "    ids = ids[:BLOCK_SIZE]\n",
        "    x = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len - len(ids)):\n",
        "            logits = model(x)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            top_logits, top_idx = torch.topk(logits, top_k)\n",
        "            probs = F.softmax(top_logits, dim=-1)\n",
        "            next_id = top_idx[0, torch.multinomial(probs, 1)[0]]\n",
        "            x = torch.cat([x, next_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "    return tokenizer.decode([i-3 for i in x[0].cpu().numpy() if i>2])\n",
        "\n",
        "# Test generation\n",
        "prompt = \"To be, or not to be\"\n",
        "print(\"\\nGenerated text:\\n\", generate_text(model, sp_tokenizer, prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_PRjffCRcyU",
        "outputId": "2084f936-b60f-428e-baa6-e6280c5582a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1:   1%|          | 148/18279 [00:06<10:31, 28.70it/s, loss=6.17]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1n2IQ2eDOTBA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-e4TfzaP9qT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}