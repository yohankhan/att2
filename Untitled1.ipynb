{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoHu5Vj0N-tk",
        "outputId": "f5aada3d-d596-4efd-a496-50ddf2a026ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Unigram tokenizer...\n",
            "Tokenizer training complete! Files: sp_unigram_25k.model / sp_unigram_25k.vocab\n",
            "\n",
            "Evaluation Results:\n",
            "Average tokens per word: 1.2885\n",
            "Max tokens in a line: 23\n",
            "Reconstruction rate: 0.9606\n",
            "\n",
            "✅ Tokenizer saved locally in Colab as: sp_unigram_25k.model and sp_unigram_25k.vocab\n",
            "You can download them using:\n",
            "files.download('sp_unigram_25k.model')\n",
            "files.download('sp_unigram_25k.vocab')\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "# !pip install sentencepiece pandas\n",
        "# !pip install tokenizers  transformers datasets\n",
        "\n",
        "import sentencepiece as spm\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "import unicodedata\n",
        "\n",
        "input_file = \"raw.txt\"\n",
        "output_file = \"shakespeare_clean.txt\"\n",
        "\n",
        "with open(input_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Unicode normalization (NFKC)\n",
        "text = unicodedata.normalize('NFKC', text)\n",
        "\n",
        "# Keep line breaks and punctuation (important for poetic structure)\n",
        "# Optional: strip trailing whitespaces\n",
        "lines = [line.rstrip() for line in text.splitlines() if line.strip()]\n",
        "\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(lines))\n",
        "\n",
        "# -------------------------------\n",
        "# PARAMETERS YOU CAN TWEAK\n",
        "# -------------------------------\n",
        "INPUT_FILE = \"shakespeare_clean.txt\"  # path to your text file\n",
        "MODEL_PREFIX = \"sp_unigram_25k\"       # model name prefix\n",
        "VOCAB_SIZE = 25000                     # increase to 32000, 50000 for larger vocab\n",
        "MODEL_TYPE = \"unigram\"                # \"unigram\" or \"bpe\"\n",
        "CHARACTER_COVERAGE = 0.9995            # 0.99 for most English text\n",
        "INPUT_SENTENCE_SIZE = 2000000          # number of sentences to sample for training\n",
        "SHUFFLE = True                         # shuffle sentences before training\n",
        "N_EVAL_LINES = 5000                     # number of lines to use for quick evaluation\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 1: Train Tokenizer\n",
        "# -------------------------------\n",
        "print(\"Training Unigram tokenizer...\")\n",
        "spm.SentencePieceTrainer.train(\n",
        "    input=INPUT_FILE,\n",
        "    model_prefix=MODEL_PREFIX,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    model_type=MODEL_TYPE,\n",
        "    character_coverage=CHARACTER_COVERAGE,\n",
        "    input_sentence_size=INPUT_SENTENCE_SIZE,\n",
        "    shuffle_input_sentence=SHUFFLE\n",
        ")\n",
        "print(f\"Tokenizer training complete! Files: {MODEL_PREFIX}.model / {MODEL_PREFIX}.vocab\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 2: Load Tokenizer\n",
        "# -------------------------------\n",
        "sp_tokenizer = spm.SentencePieceProcessor(model_file=f\"{MODEL_PREFIX}.model\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 3: Evaluation Function\n",
        "# -------------------------------\n",
        "def evaluate_tokenizer(sp_model, text_path, n_lines=None):\n",
        "    total_tokens, total_words, recon_errors, max_len = 0, 0, 0, 0\n",
        "    with open(text_path, 'r', encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if n_lines and i >= n_lines:\n",
        "                break\n",
        "            line = line.rstrip(\"\\n\")\n",
        "            if not line:\n",
        "                continue\n",
        "            enc = sp_model.encode(line, out_type=int)\n",
        "            decoded = sp_model.decode(enc).strip()\n",
        "            if decoded != line.strip():\n",
        "                recon_errors += 1\n",
        "            total_tokens += len(enc)\n",
        "            total_words += len(line.split())\n",
        "            max_len = max(max_len, len(enc))\n",
        "    return {\n",
        "        \"avg_tokens_per_word\": total_tokens / (total_words + 1e-9),\n",
        "        \"max_tokens_in_line\": max_len,\n",
        "        \"reconstruction_rate\": 1 - recon_errors / (i+1e-9)\n",
        "    }\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 4: Run Evaluation\n",
        "# -------------------------------\n",
        "results = evaluate_tokenizer(sp_tokenizer, INPUT_FILE, n_lines=N_EVAL_LINES)\n",
        "print(\"\\nEvaluation Results:\")\n",
        "print(f\"Average tokens per word: {results['avg_tokens_per_word']:.4f}\")\n",
        "print(f\"Max tokens in a line: {results['max_tokens_in_line']}\")\n",
        "print(f\"Reconstruction rate: {results['reconstruction_rate']:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# STEP 5: Save Tokenizer Locally in Colab\n",
        "# -------------------------------\n",
        "print(f\"\\n✅ Tokenizer saved locally in Colab as: {MODEL_PREFIX}.model and {MODEL_PREFIX}.vocab\")\n",
        "print(\"You can download them using:\")\n",
        "print(f\"files.download('{MODEL_PREFIX}.model')\")\n",
        "print(f\"files.download('{MODEL_PREFIX}.vocab')\")\n",
        "# 1. Average Tokens per Word\n",
        "\n",
        "# What it tells you: How efficiently the tokenizer compresses text into tokens.\n",
        "\n",
        "# Lower is better → fewer tokens per word means your model will process longer context for the same input size.\n",
        "\n",
        "# Compare: If your new tokenizer has lower or similar avg tokens/word than 1.3085, it’s more efficient or equally efficient.\n",
        "\n",
        "# 2. Max Tokens per Line\n",
        "\n",
        "# What it tells you: The longest tokenized sequence in a single line.\n",
        "\n",
        "# Lower is generally better → prevents very long sequences that could slow training or require truncation.\n",
        "\n",
        "# Compare: If your new tokenizer reduces this from 24, it’s better in sequence efficiency.\n",
        "\n",
        "# 3. Reconstruction Rate\n",
        "\n",
        "# What it tells you: How accurately the tokenizer can reproduce the original text after encoding → decoding.\n",
        "\n",
        "# Higher is better → close to 1 (or 100%) means no loss of information.\n",
        "\n",
        "# Compare: Your current tokenizer has 0.9606 → new tokenizer should ideally be ≥ 0.9606.\n",
        "\n",
        "# Summary Table for Evaluation\n",
        "# Metric\tBetter Condition\n",
        "# Avg Tokens per Word\tLower than 1.3085\n",
        "# Max Tokens per Line\tLower than 24\n",
        "# Reconstruction Rate\tEqual to or higher than 0.9606\n",
        "\n",
        "# ✅ Rule of Thumb:\n",
        "\n",
        "# If average tokens per word decreases without dropping reconstruction rate, your tokenizer is better.\n",
        "\n",
        "# Slight increase in max tokens per line is okay if reconstruction improves or avg tokens per word decreases.\n",
        "\n",
        "# If reconstruction rate drops significantly, the tokenizer is losing fidelity, even if token counts improve.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE3mbqiKReOD",
        "outputId": "bbd5f39f-c912-4acb-9515-9a60ee205af9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ===============================\n",
        "# !pip install torch einops sentencepiece datasets accelerate tqdm --quiet\n",
        "\n",
        "# ===============================\n",
        "# IMPORTS\n",
        "# ===============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sentencepiece as spm\n",
        "from datasets import Dataset as HFDataset\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "from einops import rearrange\n",
        "\n",
        "# ===============================\n",
        "# PARAMETERS (Tweak for experiments)\n",
        "# ===============================\n",
        "# Model parameters\n",
        "VOCAB_SIZE = 25000            # Must match tokenizer\n",
        "BLOCK_SIZE = 20           # Context window\n",
        "EMBED_DIM = 512               # Embedding dimension\n",
        "NUM_LAYERS = 6                # Transformer layers\n",
        "NUM_HEADS = 8                 # Attention heads\n",
        "DROPOUT = 0.1                 # Dropout\n",
        "FF_DIM = 4*EMBED_DIM          # Feed-forward hidden dimension\n",
        "\n",
        "# Training parameters\n",
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 5e-4\n",
        "GRAD_CLIP = 1.0\n",
        "EVAL_INTERVAL = 500\n",
        "SAVE_INTERVAL = 1000\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Tokenizer\n",
        "TOKENIZER_MODEL = \"sp_unigram_25k.model\"\n",
        "TEXT_FILE = \"shakespeare_clean.txt\"\n",
        "\n",
        "# ===============================\n",
        "# STEP 1: LOAD TOKENIZER\n",
        "# ===============================\n",
        "tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL)\n",
        "\n",
        "# ===============================\n",
        "# STEP 2: PREPARE DATASET\n",
        "# ===============================\n",
        "class LMDataset(Dataset):\n",
        "    def __init__(self, text_file, tokenizer, block_size):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.data = []\n",
        "        with open(text_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    ids = tokenizer.encode(line, out_type=int)\n",
        "                    for i in range(0, len(ids)-block_size, block_size):\n",
        "                        chunk = ids[i:i+block_size+1]  # +1 for next token prediction\n",
        "                        self.data.append(torch.tensor(chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx][:-1]\n",
        "        y = self.data[idx][1:]\n",
        "        return x, y\n",
        "\n",
        "dataset = LMDataset(TEXT_FILE, tokenizer, BLOCK_SIZE)\n",
        "train_size = int(0.95 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# STEP 3: TRANSFORMER WITH RoPE & FLASH ATTENTION\n",
        "# ===============================\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "\n",
        "    def forward(self, seq_len, device):\n",
        "        t = torch.arange(seq_len, device=device).type_as(self.inv_freq)\n",
        "        freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "        return emb\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, freqs):\n",
        "    cos, sin = freqs.cos(), freqs.sin()\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
        "    return torch.cat([-x2, x1], dim=-1)\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.qkv = nn.Linear(embed_dim, 3*embed_dim)\n",
        "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rope = RotaryEmbedding(self.head_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv(x).view(B, T, 3, self.num_heads, self.head_dim)\n",
        "        q, k, v = qkv[:,:,0], qkv[:,:,1], qkv[:,:,2]\n",
        "        q = q.transpose(1,2); k = k.transpose(1,2); v = v.transpose(1,2)\n",
        "        freqs = self.rope(T, x.device)\n",
        "        q, k = apply_rotary_pos_emb(q, k, freqs)\n",
        "        attn = torch.matmul(q, k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
        "        mask = torch.tril(torch.ones(T, T, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
        "        attn = attn.masked_fill(mask==0, float('-inf'))\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1,2).contiguous().view(B,T,C)\n",
        "        return self.proj(out)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads, dropout)\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, block_size, embed_dim, num_layers, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
        "    def forward(self, idx):\n",
        "        B, T = idx.size()\n",
        "        pos = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        x = self.token_emb(idx) + self.pos_emb(pos)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "model = TransformerLM(VOCAB_SIZE, BLOCK_SIZE, EMBED_DIM, NUM_LAYERS, NUM_HEADS, FF_DIM, DROPOUT).to(DEVICE)\n",
        "\n",
        "# Enable mixed precision\n",
        "scaler = torch.cuda.amp.GradScaler() if DEVICE==\"cuda\" else None\n",
        "\n",
        "# ===============================\n",
        "# STEP 4: OPTIMIZER AND LOSS\n",
        "# ===============================\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ===============================\n",
        "# STEP 5: TRAINING LOOP\n",
        "# ===============================\n",
        "global_step = 0\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        if scaler:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "            optimizer.step()\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "        if global_step % EVAL_INTERVAL == 0:\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for vx, vy in val_loader:\n",
        "                    vx, vy = vx.to(DEVICE), vy.to(DEVICE)\n",
        "                    logits = model(vx)\n",
        "                    val_loss += criterion(logits.view(-1, VOCAB_SIZE), vy.view(-1)).item()\n",
        "            val_loss /= len(val_loader)\n",
        "            print(f\"\\nStep {global_step} | Eval Loss: {val_loss:.4f} | Perplexity: {math.exp(val_loss):.2f}\")\n",
        "            model.train()\n",
        "\n",
        "        if global_step % SAVE_INTERVAL == 0:\n",
        "            torch.save(model.state_dict(), f\"transformer_lm_step{global_step}.pt\")\n",
        "            print(f\"Checkpoint saved at step {global_step}\")\n",
        "\n",
        "# ===============================\n",
        "# STEP 6: TEXT GENERATION FUNCTION\n",
        "# ===============================\n",
        "def generate_text(model, tokenizer, prompt, max_len=BLOCK_SIZE, top_k=50, top_p=0.95):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(tokenizer.encode(prompt, out_type=int), device=DEVICE).unsqueeze(0)\n",
        "    for _ in range(max_len):\n",
        "        logits = model(idx)\n",
        "        logits = logits[:, -1, :]\n",
        "        # Top-k and Top-p filtering\n",
        "        topk = torch.topk(logits, top_k, dim=-1)\n",
        "        filtered_logits = torch.full_like(logits, -float('Inf'))\n",
        "        filtered_logits.scatter_(-1, topk.indices, topk.values)\n",
        "        probs = F.softmax(filtered_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_token], dim=1)\n",
        "    return tokenizer.decode(idx[0].cpu().numpy())\n",
        "\n",
        "# Example generation\n",
        "prompt = \"To be, or not to be\"\n",
        "print(\"\\nGenerated text:\\n\", generate_text(model, tokenizer, prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "O_PRjffCRcyU",
        "outputId": "aae411a7-9975-48c6-8dba-a5f5370947c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1119822222.py:179: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if DEVICE==\"cuda\" else None\n",
            "Epoch 1:   0%|          | 0/40 [00:00<?, ?it/s]/tmp/ipython-input-1119822222.py:197: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1: 100%|██████████| 40/40 [00:02<00:00, 14.86it/s]\n",
            "Epoch 2: 100%|██████████| 40/40 [00:01<00:00, 23.37it/s]\n",
            "Epoch 3: 100%|██████████| 40/40 [00:01<00:00, 23.84it/s]\n",
            "Epoch 4: 100%|██████████| 40/40 [00:01<00:00, 23.70it/s]\n",
            "Epoch 5: 100%|██████████| 40/40 [00:01<00:00, 23.62it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py:125: UserWarning: gemm_and_bias error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 1 transpose_mat2 0 m 512 n 21 k 2048 mat1_ld 2048 mat2_ld 2048 result_ld 512 abType 0 cType 0 computeType 68 scaleType 0. Will attempt to recover by calling unfused cublas path. (Triggered internally at /pytorch/aten/src/ATen/cuda/CUDABlas.cpp:1709.)\n",
            "  return F.linear(input, self.weight, self.bias)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1119822222.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;31m# Example generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"To be, or not to be\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nGenerated text:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1119822222.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, max_len, top_k, top_p)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;31m# Top-k and Top-p filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1119822222.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1119822222.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1119822222.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTransformerBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ===============================\n",
        "# !pip install torch sentencepiece tqdm --quiet\n",
        "\n",
        "# ===============================\n",
        "# IMPORTS\n",
        "# ===============================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sentencepiece as spm\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ===============================\n",
        "# PARAMETERS (Tweak for experiments)\n",
        "# ===============================\n",
        "INPUT_FILE = \"shakespeare_clean.txt\"\n",
        "TOKENIZER_MODEL = \"sp_unigram_25k.model\"\n",
        "\n",
        "VOCAB_SIZE = 25000         # tokenizer vocab\n",
        "BLOCK_SIZE = 128           # context window\n",
        "NUM_LAYERS = 6\n",
        "NUM_HEADS = 8\n",
        "EMBED_DIM = 512\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_EPOCHS = 3\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MAX_SEQ_LEN = BLOCK_SIZE\n",
        "\n",
        "# ===============================\n",
        "# STEP 1: LOAD TOKENIZER\n",
        "# ===============================\n",
        "sp_tokenizer = spm.SentencePieceProcessor(model_file=TOKENIZER_MODEL)\n",
        "\n",
        "BOS_ID = sp_tokenizer.bos_id()\n",
        "EOS_ID = sp_tokenizer.eos_id()\n",
        "PAD_ID = sp_tokenizer.pad_id()\n",
        "\n",
        "# Safety clamp\n",
        "BOS_ID = min(BOS_ID if BOS_ID != -1 else 0, VOCAB_SIZE - 1)\n",
        "EOS_ID = min(EOS_ID if EOS_ID != -1 else 1, VOCAB_SIZE - 1)\n",
        "PAD_ID = min(PAD_ID if PAD_ID != -1 else 2, VOCAB_SIZE - 1)\n",
        "\n",
        "print(\"Tokenizer IDs -> BOS:\", BOS_ID, \"EOS:\", EOS_ID, \"PAD:\", PAD_ID)\n",
        "\n",
        "# ===============================\n",
        "# STEP 2: LOAD DATASET\n",
        "# ===============================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.block_size = block_size\n",
        "        self.lines = []\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    ids = tokenizer.encode(line, out_type=int)\n",
        "                    # Truncate long sequences\n",
        "                    if len(ids) > block_size:\n",
        "                        ids = ids[:block_size]\n",
        "                    self.lines.append(ids)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ids = self.lines[idx]\n",
        "        # Pad sequence\n",
        "        padding_len = self.block_size - len(ids)\n",
        "        ids = ids + [PAD_ID] * padding_len\n",
        "        return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(INPUT_FILE, sp_tokenizer, BLOCK_SIZE)\n",
        "train_size = int(0.95 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# ===============================\n",
        "# STEP 3: DEFINE TRANSFORMER LM\n",
        "# ===============================\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, block_size, pad_id):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n",
        "        self.pos_emb = nn.Embedding(block_size, embed_dim)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=embed_dim,\n",
        "                nhead=num_heads,\n",
        "                dim_feedforward=embed_dim * 4,\n",
        "                activation=\"gelu\",\n",
        "                batch_first=True\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        pos_ids = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        h = self.token_emb(x) + self.pos_emb(pos_ids)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h)\n",
        "        h = self.ln_f(h)\n",
        "        logits = self.head(h)\n",
        "        return logits\n",
        "\n",
        "model = TransformerLM(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, BLOCK_SIZE, PAD_ID).to(DEVICE)\n",
        "\n",
        "# ===============================\n",
        "# STEP 4: TRAINING LOOP\n",
        "# ===============================\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "scaler = torch.amp.GradScaler() if DEVICE == \"cuda\" else None\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, scaler):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(loader, desc=\"Training\"):\n",
        "        batch = batch.to(DEVICE)\n",
        "        inputs = batch[:, :-1]\n",
        "        targets = batch[:, 1:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with torch.amp.autocast(enabled=scaler is not None):\n",
        "            logits = model(inputs)\n",
        "            loss = loss_fn(logits.view(-1, VOCAB_SIZE), targets.view(-1))\n",
        "        if scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(DEVICE)\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "            logits = model(inputs)\n",
        "            loss = loss_fn(logits.view(-1, VOCAB_SIZE), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, scaler)\n",
        "    val_loss = evaluate(model, val_loader)\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Perplexity: {math.exp(val_loss):.2f}\")\n",
        "\n",
        "# ===============================\n",
        "# STEP 5: SAMPLE GENERATION\n",
        "# ===============================\n",
        "def generate_text(model, tokenizer, prompt, max_len=BLOCK_SIZE, temperature=1.0, top_k=50):\n",
        "    model.eval()\n",
        "    ids = tokenizer.encode(prompt, out_type=int)\n",
        "    ids = [BOS_ID] + ids\n",
        "    input_ids = torch.tensor(ids, device=DEVICE).unsqueeze(0)\n",
        "    for _ in range(max_len):\n",
        "        logits = model(input_ids)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        top_logits, top_idx = torch.topk(logits, top_k)\n",
        "        probs = F.softmax(top_logits, dim=-1)\n",
        "        next_id = top_idx[0, torch.multinomial(probs, num_samples=1)]\n",
        "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        if next_id.item() == EOS_ID:\n",
        "            break\n",
        "    return tokenizer.decode(input_ids[0].cpu().numpy())\n",
        "\n",
        "# Example\n",
        "prompt = \"To be, or not to be\"\n",
        "print(\"\\nGenerated text:\\n\", generate_text(model, sp_tokenizer, prompt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "1n2IQ2eDOTBA",
        "outputId": "11b9e72c-f681-445d-dd88-e4449a4a8877"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer IDs -> BOS: 1 EOS: 2 PAD: 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AcceleratorError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1043753772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformerLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_HEADS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBLOCK_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPAD_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m# ===============================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     )\n\u001b[0;32m-> 1355\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1356\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-e4TfzaP9qT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}